{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Iniciamos o projeto importando e extraindo o dataset que está compactado (.zip) no Google Drive. Nesse caso, estamos importando o dataset já contendo as pastas de planejamento, preprocessamento e treinamento afim de realizar a inferência. (O funcionamento é o mesmo caso queira utilizar um dataset novo contendo apenas as dependências originais explicadas à frente)."
      ],
      "metadata": {
        "id": "mIJtvemgqa3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDMtMS17PoXL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "raw_data_zip_path = '/content/drive/MyDrive/nnUNet_raw_data_base.zip'\n",
        "preprocessed_zip_path = '/content/drive/MyDrive/nnUNet_preprocessed.zip'\n",
        "results_zip_path = '/content/drive/MyDrive/nnUNet_results.zip'\n",
        "\n",
        "extract_base_dir = '/content'\n",
        "os.makedirs(extract_base_dir, exist_ok=True)\n",
        "\n",
        "def extract_and_fix(zip_file_path, target_extract_dir):\n",
        "    print(f\"\\nExtracting {os.path.basename(zip_file_path)} to {target_extract_dir}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(target_extract_dir)\n",
        "        print(f\"{os.path.basename(zip_file_path)} extracted.\")\n",
        "        nested_content_dir = os.path.join(target_extract_dir, 'content')\n",
        "        if os.path.exists(nested_content_dir) and os.path.isdir(nested_content_dir) and os.listdir(nested_content_dir):\n",
        "            print(f\"Detected nested '/content/content' directory. Moving contents up...\")\n",
        "            for item in os.listdir(nested_content_dir):\n",
        "                shutil.move(os.path.join(nested_content_dir, item), target_extract_dir)\n",
        "            shutil.rmtree(nested_content_dir)\n",
        "            print(\"Contents moved successfully. Nested folder removed.\")\n",
        "        else:\n",
        "            print(\"No nested '/content/content' detected for this zip.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {zip_file_path} not found. Please check the path in your Google Drive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during extraction of {os.path.basename(zip_file_path)}: {e}\")\n",
        "extract_and_fix(raw_data_zip_path, extract_base_dir)\n",
        "extract_and_fix(preprocessed_zip_path, extract_base_dir)\n",
        "extract_and_fix(results_zip_path, extract_base_dir)\n",
        "\n",
        "print(\"\\n--- Final Verification of Extracted Folders ---\")\n",
        "expected_final_dirs = [\n",
        "    '/content/nnUNet_raw_data_base',\n",
        "    '/content/nnUNet_preprocessed',\n",
        "    '/content/nnUNet_results'\n",
        "]\n",
        "\n",
        "for d in expected_final_dirs:\n",
        "    if os.path.exists(d):\n",
        "        print(f\"Found: {d}\")\n",
        "        if 'nnUNet_raw_data_base' in d:\n",
        "            dataset_path = os.path.join(d, 'nnUNet_raw_data', 'Dataset505_BraTS2020_subset')\n",
        "            if os.path.exists(dataset_path):\n",
        "                print(f\"   --> Found Dataset505_BraTS2020_subset at: {dataset_path}\")\n",
        "                images_ts_path = os.path.join(dataset_path, 'imagesTs')\n",
        "                if os.path.exists(images_ts_path) and os.listdir(images_ts_path):\n",
        "                    print(f\"   --> Confirmed imagesTs folder exists and is not empty.\")\n",
        "                else:\n",
        "                    print(f\"   --> WARNING: imagesTs folder not found or empty at: {images_ts_path}\")\n",
        "            else:\n",
        "                print(f\"   --> WARNING: Dataset505_BraTS2020_subset not found inside {d}.\")\n",
        "    else:\n",
        "        print(f\"NOT FOUND: {d}. Something went wrong with the extraction or fix.\")\n",
        "\n",
        "print(\"\\nSetup for inference complete! You can now proceed with setting environment variables and running prediction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação das dependências e da nnUNet através do github:"
      ],
      "metadata": {
        "id": "41WH1HhiqNaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "!git clone https://github.com/MIC-DKFZ/nnUNet.git\n",
        "\n",
        "%cd nnUNet\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "RfFtebLPQ5jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de variáveis de ambiente (padrão para o nnUNetv2)."
      ],
      "metadata": {
        "id": "ygFKC2YSq9aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['nnUNet_preprocessed'] = '/content/nnUNet_preprocessed'\n",
        "os.environ['nnUNet_results'] = '/content/nnUNet_results'\n",
        "os.environ['nnUNet_raw'] = '/content/nnUNet_raw_data_base'"
      ],
      "metadata": {
        "id": "yxs2WQlRSCR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "input_images_ts_dir = '/content/nnUNet_raw_data_base/nnUNet_raw_data/Dataset505_BraTS2020_subset/imagesTs'\n",
        "output_predictions_dir = '/content/nnUNet_predictions'\n",
        "os.makedirs(output_predictions_dir, exist_ok=True)\n",
        "print(f\"Output directory for predictions created: {output_predictions_dir}\")\n",
        "task_id = 505\n",
        "model_config = '3d_fullres'\n",
        "folds_to_predict = 0\n",
        "print(f\"\\nStarting nnUNet prediction for Task {task_id} with {model_config} model...\")\n",
        "print(f\"Input: {input_images_ts_dir}\")\n",
        "print(f\"Output: {output_predictions_dir}\")"
      ],
      "metadata": {
        "id": "52tS-RfbZ70Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover o conteúdo extraído para o local correto"
      ],
      "metadata": {
        "id": "JqCBVVKAqIqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/nnUNet_raw_data_base/nnUNet_raw_data/Dataset505_BraTS2020_subset /content/nnUNet_raw_data_base/"
      ],
      "metadata": {
        "id": "fQxql69R5KKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para executar o planejamento e o preprocessamento dos dados, as pastas devem conter a seguinte estrutura:\n",
        "/content/nnUNet_raw_data_base/(Dataset)/imagesTr                                \n",
        "/content/nnUNet_raw_data_base/(Dataset)/labelsTr                                \n",
        "/content/nnUNet_raw_data_base/(Dataset)/dataset.json"
      ],
      "metadata": {
        "id": "MzT6p37vpQL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código também realiza a verificação da integridade do dataset."
      ],
      "metadata": {
        "id": "Re0V2avCrFw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_plan_and_preprocess -d 505 --verify_dataset_integrity"
      ],
      "metadata": {
        "id": "OYOGqhWy19Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso queira dar início ao treinamento:"
      ],
      "metadata": {
        "id": "lGpgZPFWz02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 505 3d_fullres 0"
      ],
      "metadata": {
        "id": "CuKM5SghzuuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso queira retomar o treinamento:"
      ],
      "metadata": {
        "id": "NVwSKBRKz6sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 505 3d_fullres 0 --c"
      ],
      "metadata": {
        "id": "PFqNCy2Szo2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizar a inferência baseada nos passos realizados acima. Note que no mesmo local onde existem as pastas imagesTr e labelsTr, deve haver as pastas imagesTs e labelsTs para teste."
      ],
      "metadata": {
        "id": "zhyTjclUrLKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_predict \\\n",
        "  -i /content/nnUNet_raw_data_base/Dataset505_BraTS2020_subset/imagesTs \\\n",
        "  -o /content/nnUNet_raw_data_base/nnUNet_raw_data/Dataset505_BraTS2020_subset/predictions \\\n",
        "  -d Dataset505_BraTS2020_subset \\\n",
        "  -c 3d_fullres \\\n",
        "  -f 0 \\\n",
        "  -chk checkpoint_best.pth"
      ],
      "metadata": {
        "id": "GZKWUGTGdPYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código a seguir realiza uma plotagem dos dados obtidos acima para melhor visualização de desempenho e resultados conclusivos."
      ],
      "metadata": {
        "id": "axlOEFYKrgSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "json_path = \"/content/nnUNet_raw_data_base/Dataset505_BraTS2020_subset/predictions/summary.json\"\n",
        "with open(json_path) as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "df_mean = pd.DataFrame.from_dict(summary[\"mean\"], orient=\"index\")\n",
        "df_mean.index.name = \"Classe\"\n",
        "df_mean.reset_index(inplace=True)\n",
        "\n",
        "print(\"MÉTRICAS MÉDIAS POR CLASSE (ORIGINAL)\")\n",
        "display(df_mean)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(df_mean['Classe'].astype(str), df_mean['Dice'], color='skyblue')\n",
        "plt.title(\"Dice Score por Classe (Média Geral)\")\n",
        "plt.xlabel(\"Classe\")\n",
        "plt.ylabel(\"Dice\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "df_mean[[\"Classe\", \"Dice\", \"IoU\", \"FP\", \"FN\"]].set_index(\"Classe\").plot(\n",
        "    kind=\"bar\", figsize=(10, 6), title=\"Comparação de Métricas por Classe (Média Geral)\"\n",
        ")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "worst_case, best_case = None, None\n",
        "worst_dice, best_dice = float(\"inf\"), float(\"-inf\")\n",
        "case_dice_averages = []\n",
        "\n",
        "for case in summary[\"metric_per_case\"]:\n",
        "    metrics = case[\"metrics\"]\n",
        "    mean_dice = sum(m[\"Dice\"] for m in metrics.values()) / len(metrics)\n",
        "    case_dice_averages.append((mean_dice, case))\n",
        "    if mean_dice < worst_dice:\n",
        "        worst_dice = mean_dice\n",
        "        worst_case = case\n",
        "    if mean_dice > best_dice:\n",
        "        best_dice = mean_dice\n",
        "        best_case = case\n",
        "\n",
        "df_worst = pd.DataFrame.from_dict(worst_case[\"metrics\"], orient=\"index\")\n",
        "df_worst.index.name = \"Classe\"\n",
        "df_worst.reset_index(inplace=True)\n",
        "\n",
        "print(f\"\\nPIOR CASO: {worst_case['prediction_file'].split('/')[-1]} | Média Dice: {worst_dice:.4f}\")\n",
        "display(df_worst)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(df_worst['Classe'].astype(str), df_worst['Dice'], color='salmon')\n",
        "plt.title(\"Dice Score por Classe (Pior Caso)\")\n",
        "plt.xlabel(\"Classe\")\n",
        "plt.ylabel(\"Dice\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "df_worst[[\"Classe\", \"Dice\", \"IoU\", \"FP\", \"FN\"]].set_index(\"Classe\").plot(\n",
        "    kind=\"bar\", figsize=(10, 6), title=\"Comparação de Métricas por Classe (Pior Caso)\"\n",
        ")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "df_best = pd.DataFrame.from_dict(best_case[\"metrics\"], orient=\"index\")\n",
        "df_best.index.name = \"Classe\"\n",
        "df_best.reset_index(inplace=True)\n",
        "\n",
        "print(f\"\\nMELHOR CASO: {best_case['prediction_file'].split('/')[-1]} | Média Dice: {best_dice:.4f}\")\n",
        "display(df_best)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(df_best['Classe'].astype(str), df_best['Dice'], color='mediumseagreen')\n",
        "plt.title(\"Dice Score por Classe (Melhor Caso)\")\n",
        "plt.xlabel(\"Classe\")\n",
        "plt.ylabel(\"Dice\")\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "df_best[[\"Classe\", \"Dice\", \"IoU\", \"FP\", \"FN\"]].set_index(\"Classe\").plot(\n",
        "    kind=\"bar\", figsize=(10, 6), title=\"Comparação de Métricas por Classe (Melhor Caso)\"\n",
        ")\n",
        "plt.ylabel(\"Valor\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "27u7KMzMhbmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código a seguir também utiliza de meios visuais para fazermos a análise dos resultados, porém utilizando os dados puramente do treinamento."
      ],
      "metadata": {
        "id": "O4YJdlnkrsDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_file_path = \"/content/nnUNet_results/Dataset505_BraTS2020_subset/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/training_log_2025_6_3_17_58_45.txt\"\n",
        "\n",
        "epochs = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "learning_rates = []\n",
        "epoch_times = []\n",
        "dice_0, dice_1, dice_2 = [], [], []\n",
        "\n",
        "with open(log_file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        if \"Epoch \" in line:\n",
        "            match = re.search(r\"Epoch (\\d+)\", line)\n",
        "            if match:\n",
        "                epochs.append(int(match.group(1)))\n",
        "        if \"Current learning rate\" in line:\n",
        "            match = re.search(r\"Current learning rate: ([\\d.]+)\", line)\n",
        "            if match:\n",
        "                learning_rates.append(float(match.group(1)))\n",
        "        if \"train_loss\" in line:\n",
        "            match = re.search(r\"train_loss (-?[\\d.]+)\", line)\n",
        "            if match:\n",
        "                train_losses.append(float(match.group(1)))\n",
        "        if \"val_loss\" in line:\n",
        "            match = re.search(r\"val_loss (-?[\\d.]+)\", line)\n",
        "            if match:\n",
        "                val_losses.append(float(match.group(1)))\n",
        "        if \"Pseudo dice\" in line:\n",
        "            match = re.search(r\"\\[np\\.float32\\(([\\d.]+)\\), np\\.float32\\(([\\d.]+)\\), np\\.float32\\(([\\d.]+)\\)\\]\", line)\n",
        "            if match:\n",
        "                dice_0.append(float(match.group(1)))\n",
        "                dice_1.append(float(match.group(2)))\n",
        "                dice_2.append(float(match.group(3)))\n",
        "        if \"Epoch time\" in line:\n",
        "            match = re.search(r\"Epoch time: ([\\d.]+) s\", line)\n",
        "            if match:\n",
        "                epoch_times.append(float(match.group(1)))\n",
        "\n",
        "def plotar(x, y_list, labels, title, ylabel):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    min_len = min(len(x), *(len(y) for y in y_list))\n",
        "    x = x[:min_len]\n",
        "    y_list = [y[:min_len] for y in y_list]\n",
        "    for y, label in zip(y_list, labels):\n",
        "        plt.plot(x, y, label=label)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plotar(epochs, [train_losses, val_losses], [\"Train Loss\", \"Val Loss\"], \"Train vs Val Loss\", \"Loss\")\n",
        "plotar(epochs, [dice_0, dice_1, dice_2], [\"Class 0\", \"Class 1\", \"Class 2\"], \"Pseudo Dice per Class\", \"Dice Score\")\n",
        "plotar(epochs, [learning_rates], [\"Learning Rate\"], \"Learning Rate over Epochs\", \"Learning Rate\")\n",
        "plotar(epochs, [epoch_times], [\"Epoch Time\"], \"Time per Epoch\", \"Seconds\")"
      ],
      "metadata": {
        "id": "i9KOX8HXhyQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}